import weave

# Weave Setup

WEAVE_PROJECT = "hackercup"  # REPLACE WITH YOUR PROJECT NAME
weave_client = weave.init(WEAVE_PROJECT)


from utils import (
    # FAST_LLM, #change
    # STRONG_LLM, #change
    Problem,
    Solution,
    tokenizer,
    model,
    # async_client,
    check_correctness,
    format_response,
)


import asyncio
import logging

from nest_asyncio import apply

apply()

# Some logging to see the progress
logging.basicConfig(
    format="%(asctime)s : %(levelname)s : %(message)s", level=logging.INFO
)

logger = logging.getLogger(__name__)


from agent import SOLVER_INSTRUCTIONS, draft_solution, zero_shot_solver, rag_solver
import torch
# print(SOLVER_INSTRUCTIONS)


# @weave.op
# async def draft_solution(
#         problem: Problem, temperature: float = 0.0
# ) -> Solution:
#     user_prompt = f"""{problem.as_xml}
# ---
# Let's think step by step to solve the problem:
# """

#     messages = [
#         {"role": "system", "content": SOLVER_INSTRUCTIONS},
#         {"role": "user", "content": user_prompt},
#     ]
    
#     inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(model.device)
    
#     with torch.no_grad():
#         outputs = model.generate(
#             inputs, 
#             max_new_tokens=512, 
#             do_sample=(temperature > 0),
#             temperature=temperature,
#             top_k=50, 
#             top_p=0.95, 
#             num_return_sequences=1, 
#             eos_token_id=tokenizer.eos_token_id
#         )
    
#     response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)
    

#     formatted_response = await format_response(
#         response, Solution
#     )
    
#     return formatted_response

# @weave.op
# async def zero_shot_solver(
#         problem: Problem, model: str = FAST_LLM, temperature: float = 0.0, timeout: int = 10
# ) -> dict:
#     logger.info("Drafting intial zero-shot solution")
#     solution = await draft_solution(
#         problem=problem,
#         model=model,
#         temperature=temperature,
#     )
#     test_report = check_correctness(
#         solution.source_code, problem.sample_input, problem.sample_output, timeout
#     )
#     logger.info(f"Draft solution result: {repr(test_report)}")
#     return {"solution": solution, "test_report": test_report, "stage": "zero-shot"}


practice_dataset_uri = "weave:///parambharat/hackercup/object/practice_dataset:R35fXf9N3FE2IOesg7bRPaPAxiE9YbpirhXO9HcHs8w"
problems_dataset = weave.ref(practice_dataset_uri).get().rows[:]
problems = list(map(lambda x: Problem(**x), problems_dataset))
problem = problems[0]

# test the zero-shot agent on the sample problem



import asyncio

class ZeroshotAgent(weave.Model):
    temperature: float = 0.0
    timeout: int = 30

    @weave.op
    async def predict(self, problem: Problem):
        return await zero_shot_solver(
            Problem(**problem),
            temperature=self.temperature,
            timeout=self.timeout,
        )
async def EvalZeroShot(eval):
    # Evaluate the zero shot agent for all the models and temperatures
    eval_temperatures = [0.0, 0.5, 1.0]
    tasks = []

    for temperature in eval_temperatures:
        zeroshot_agent = ZeroshotAgent(temperature=temperature, timeout=30)
        zeroshot_results = eval.evaluate(zeroshot_agent)
        tasks.append(zeroshot_results)

    # Phew that's 2(models)*3(temps)*5(problems) = 30 evaluations

    zeroshot_results = await asyncio.gather(*tasks)
    return zeroshot_results

async def main():

    practice_dataset_uri = "weave:///parambharat/hackercup/object/practice_dataset:R35fXf9N3FE2IOesg7bRPaPAxiE9YbpirhXO9HcHs8w"
    problems_dataset = weave.ref(practice_dataset_uri).get().rows[:]
    problems = list(map(lambda x: Problem(**x), problems_dataset))
    problem = problems[0]

    # test the zero-shot agent on the sample problem
    # zero_shot_result = await zero_shot_solver(problem)
    # print("*" * 80)
    # print(zero_shot_result["solution"].source_code)
    # print("*" * 80)
    # print(zero_shot_result["test_report"])

    # This is a simple depection of the evaluation.
    # We expect the output to be `"passed"` for all the problems if the agent is working correctly.
    examples = [{"problem": problem, "expected": "passed"} for problem in problems]


    # A simple scorer that checks if the code generated by agent passed the test case
    @weave.op
    def scorer(expected: str, model_output: dict) -> dict:
        return {"passed": expected == model_output["test_report"]}


    # This is a simple evaluation that checks if the code generated by agent passed the test
    eval = weave.Evaluation(dataset=examples, scorers=[scorer])

    # print(EvalZeroShot(eval))


    from agent import describe_examples, format_examples, generate_solution
    from retriever import Retriever, rerank_docs

    logger.info("Loading retriever ... this may take a while ...")
    retriever = Retriever()

    rag_result = await rag_solver(retriever, problem, timeout=30)
    print("*" * 80)
    print(rag_result["solution"].source_code)
    print("*" * 80)
    print(rag_result["test_report"])    






# Run the async function
asyncio.run(main())